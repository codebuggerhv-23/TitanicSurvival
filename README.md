ğŸš¢ Titanic Survival Prediction â€” Machine Learning Project
A data science project predicting passenger survival on the RMS Titanic using supervised machine learning.
This project demonstrates an end-to-end ML workflow, including preprocessing, analysis, feature engineering, scaling, regularization, and evaluation â€” all implemented in Python using Pandas, NumPy, Seaborn, and Scikit-learn.

ğŸ“˜ 1. Project Overview
The goal of this project is to analyze Titanic passenger data, extract meaningful features, and build a predictive model that estimates the likelihood of survival based on demographic and travel-related attributes.

This serves as a classic example of a binary classification problem in machine learning.

ğŸ¯ 2. Objectives
Perform complete data cleaning and exploratory analysis on the Titanic dataset.
Apply feature engineering and scaling for better model performance.
Train and optimize a classification model using regularization techniques.
Interpret the model and identify influential features affecting survival.
ğŸ§© 3. Dataset Description
Source: Kaggle â€” Titanic: Machine Learning from Disaster

Target Variable:
Survived â†’ 1 = Survived, 0 = Did not survive

Key Features:

Column	Description
PassengerId	Unique identifier for each passenger
Pclass	Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)
Name	Passenger name
Sex	Gender
Age	Age in years
SibSp	Number of siblings/spouses aboard
Parch	Number of parents/children aboard
Ticket	Ticket number
Fare	Passenger fare
Cabin	Cabin number
Embarked	Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)
âš™ï¸ 4. Workflow Summary
ğŸ§¹ Step 1: Data Preprocessing
Inspected and handled missing values.
Filled missing Age with mean, Embarked with mode.
Dropped unnecessary columns (PassengerId, Name, Ticket, Cabin).
Encoded categorical features into numeric form.
ğŸ“Š Step 2: Data Analysis
Explored survival rates by gender, class, embarkation port, and age distribution.
Visualized survival trends using Seaborn countplots and barplots.
Key Findings:

Females had higher survival probability.
1st-class passengers had better survival rates.
Traveling alone decreased survival likelihood.
ğŸ§± Step 3: Data Preparation
Split data into train (80%) and test (20%) using train_test_split().
Ensured inclusion of only relevant and encoded features.
ğŸ¤– Step 4: Baseline Model
Built initial Logistic Regression model.
Evaluated accuracy, confusion matrix, and classification report.
Detected minor overfitting â†’ applied regularization.
ğŸ§¬ Step 5: Feature Engineering
Created new derived features:

FamilySize = SibSp + Parch + 1
IsAlone = 1 if FamilySize == 1, else 0
Title = extracted from the â€œNameâ€ column (Mr, Mrs, Miss, etc.)
These features enhanced interpretability and model performance.

ğŸ“ Step 6: Feature Scaling
Used StandardScaler to normalize:

Age, Fare, FamilySize
Ensured training mean â‰ˆ 0 and std â‰ˆ 1.

ğŸ§® Step 7: Model Training with Regularization
Implemented Logistic Regression with:

Ridge (L2) and Lasso (L1) regularization
Tuned C values â†’ [0.01, 0.1, 1, 10, 100]
Best performance:
C = 0.1 with Ridge (L2) â€” minimized overfitting, maximized accuracy.

Regularization	C	Train Accuracy	Test Accuracy
Ridge (L2)	0.1	~0.82	~0.77
Lasso (L1)	0.1	~0.81	~0.76
ğŸ§¾ Step 8: Final Result & Interpretation
Final Model: Logistic Regression (L2 Regularization, C=0.1)

Performance:

Accuracy â‰ˆ 77%
Precision & Recall balanced
Confusion Matrix showed majority correctly classified
Most Influential Features:

ğŸ”¼ Positive: Sex (female), Title_Mrs, Pclass (1st), Fare (high)
ğŸ”½ Negative: IsAlone, Age, FamilySize (large)
ğŸ’¡ 5. Key Takeaways
Preprocessing, feature engineering, and regularization significantly improved model performance.
Ridge regularization balanced bias and variance well.
Logistic Regression coefficients provided interpretability for survival factors.
ğŸ§° 6. Tools and Technologies
Language: Python 3.10+

Libraries Used:

pandas, numpy â€” data handling
matplotlib, seaborn â€” visualization
scikit-learn â€” model building, scaling, regularization, evaluation
ğŸš€ 7. Future Enhancements
Use ensemble models (RandomForest, XGBoost, GradientBoosting)
Apply cross-validation for stronger generalization
Add ROC-AUC and precision-recall visualization
Deploy using Flask or Streamlit for interactive prediction
ğŸ‘¨â€ğŸ’» 8. Author
Name: Dashmeet Singh Malhotra
Role: Machine Learning Enthusiast / ML Intern Applicant

Objective: To explore real-world ML workflows, build predictive systems, and develop interpretable AI models.

ğŸ“¬ Contact:

Email: [dashcodeworks@gmail.com]
LinkedIn: [https://www.linkedin.com/in/dashmeet-singh-malhotra-6a90a3325/]
GitHub: [https://github.com/Dashmeet-S-Malhotra]
